---
title: "Abstract"
author: 
  - Nazar Demianyk
  - Anastasiia Kopylova
  - Luka Kotyshchuk
  - Anna-Lena Petersen
  - Ian Stettinger
date: today
lang: de
format:
  html: default
  pdf:
    documentclass: article
    papersize: a4
    fontsize: 12pt
    number-sections: true
execute:
  echo: false
  editor: visual
bibliography: references.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---

<!-- GitHub-Info -->

**GitHub Repository:** https://github.com/NazarD01/SMNF2025-Aufsatz-GruppeA4.git

```{r}
cat("Commit Hash:", system("git rev-parse HEAD", intern = TRUE))
```

# Einleitung

# Literaturübersicht

Im Rahmen der Literaturrecherche wurden drei wissenschaftliche Beiträge identifiziert, die wesentliche Anknüpfungspunkte zu den Forschungsfragen rund um Vertrauen in KI-gestützte Desinformations-Erkennung bieten. Sie beleuchten unterschiedliche Perspektiven – von individuellen Wahrnehmungen bis hin zu gesellschaftlich-regulatorischen Rahmenbedingungen.

@hwang2017 zeigt, wie digitale Desinformation demokratische Prozesse untergräbt und wie technologische Fortschritte – insbesondere KI und Machine Learning – die Glaubwürdigkeit von Falschmeldungen erhöhen.

@iglesiaskeller2024 et al. warnen, dass staatliche Strategien nur dann Vertrauen schaffen, wenn KI-basierte Systeme transparent, grundrechtskonform und im Zusammenspiel mehrerer Akteure implementiert werden. Überzogene Sanktionen oder Intransparenz untergraben das Vertrauen.

@wang2023 belegt anhand einer groß angelegten US-Umfrage, dass individuelles Algorithmusvertrauen die Akzeptanz KI-gestützter Moderation maßgeblich prägt; liberal Eingestellte bewerten sie tendenziell positiver, technisch Versierte häufig kritischer.

Zusammenfassend verdeutlichen die drei Arbeiten, dass Vertrauen in KI-gestützte Desinformationsbekämpfung nicht nur von technischer Funktionalität, sondern auch von gesellschaftlichen, politischen und kommunikativen Rahmenbedingungen abhängt. Dieses Zusammenspiel ist entscheidend für die Weiterentwicklung und Akzeptanz von KI-Tools.

# Methode
## Qualitativer Forschungsansatz
Wir wählten einen qualitativen Ansatz, weil unsere explorative Forschungsfrage ein vertieftes Verständnis bislang wenig untersuchter Nutzungsanforderungen verlangt. Neben technischen Dimensionen betrachten wir soziale, ethische und nutzer*innenbezogene Aspekte wie Vertrauen, Transparenz und Datenschutz – Faktoren, die stark kontextabhängig sind und sich am besten mit offenen Verfahren wie Interviews oder Fokusgruppen erfassen lassen. Diese Methoden ermöglichen es, auch unerwartete Perspektiven aufzudecken, die standardisierte Fragebögen nicht abbilden könnten.

### Stichprobe
Für unsere Erhebung rekrutierten wir zwei Personen unseres Alters (Convenience Sample). Junge Erwachsene, insbesondere Studierende, nutzen Künstliche Intelligenz (KI) regelmäßig und sind in sozialen Netzwerken hoch aktiv. Entsprechend häufig begegnen sie KI-generierten Inhalten, etwa im Nachrichtenkontext. Durch die Alters- und Lebenswelt­nähe erwarten wir reichhaltige, erfahrungsbasierte Einblicke in ihr Vertrauen und ihren Unterstützungsbedarf.

### Datenerhebung
Wir haben uns für persönliche Face-to-Face-Interviews entschieden. Zunächst haben wir die Personen, die wir interviewt haben, mit der Datenschutzerklärung und Einwilligungserklärung vertraut gemacht und sichergestellt, dass sie damit einverstanden sind. Nachdem sie dem Inhalt zugestimmt hatten, haben wir mit dem Interview begonnen. Die Fragen haben wir aus den zur Verfügung gestellten Materialien übernommen, teilweise jedoch angepasst oder präzisiert. Die beiden Interviews wurden mit einem Diktiergerät aufgenommen, anschließend mithilfe des KI-Dienstes „Whisper“ transkribiert und die Transkripte manuell korrigiert.

### Datenanalyse
Für die Analyse haben wir die Themenanalyse nach @braun_clarke gewählt. Sie besteht aus sechs Phasen, die wir schrittweise analysiert, beschrieben und zusammengefasst haben. Jede/r von uns codierte zunächst unabhängig. Im Anschluss führten wir die Codes zusammen und diskutierten Abweichungen, um Interpretationssicherheit zu gewährleisten. Dieses Vorgehen resultierte in drei zentralen Themen, die im folgenden Ergebnisabschnitt detailliert dargestellt werden.

### Ergebnisse
Wir haben zwei leitfadengestützte Interviews durchgeführt und mittels der Themenanalyse nach @braun_clarke ausgewertet. Befragt wurden zwei Personen aus der Zielgruppe junger Erwachsener, konkret Studierende. Die Auswahl dieser Personengruppe erscheint besonders geeignet im Hinblick auf unsere Forschungsfrage, da insbesondere Studierende nahezu täglich mit Künstlicher Intelligenz in Berührung kommen. Sei es im akademischen Kontext, zur Informationsbeschaffung oder zur Unterstützung bei alltäglichen Aufgaben.

Diese regelmäßige Nutzung befähigt sie dazu, fundierte Einschätzungen hinsichtlich der Funktionsweise von KI sowie deren unterstützender Rolle und Einfluss auf die digitale Mediennutzung vorzunehmen. Relevanz erhält dies insbesondere im Kontext sozialer Netzwerke, die von der Mehrheit junger Menschen intensiv genutzt werden. Gerade in diesen digitalen Räumen spielt KI eine zunehmend zentrale Rolle bei der Verbreitung von Informationen, einschließlich potenziell irreführender oder falscher Inhalte.

Studierende zeichnen sich durch eine hohe Affinität zu digitalen Technologien aus und sind daher sowohl potenziell von Desinformation betroffen als auch in der Lage, die Funktionslogiken algorithmischer Systeme zu reflektieren. Aus diesen Gründen war es für unsere Forschung zentral, die Perspektive junger Menschen zu berücksichtigen, da sie in besonderem Maße mit KI konfrontiert sind und individuell unterschiedliche Strategien im Umgang mit ihr entwickeln.

| Thema | Definition | Zitate |
|---|---|---|
| Erklärbarkeit schafft Vertrauen | Transparente Offenlegung der KI-Entscheidungskriterien wird als essenziell eingestuft. | „Das System sollte logisch arbeiten und nachvollziehbar machen, weshalb es bestimmte Informationen auf eine bestimmte Weise eingeordnet hat.“ (P, Z.129-131) „Wenn ein Post Misinformation ist [...], es dann auch anzeigt, dass es Misinformation ist [...]." (P2, Z.72-75) |
| Neutralität und Unabhängigkeit | KI muss als unparteiisch wahrgenommen werden. | „Es darf keinerlei äußeren Einfluss haben.“ (P, Z.132) „Insbesondere große Konzerne sollten nicht zu viel Einfluss darauf haben.“ (P, Z.174-176) |
| Nutzerkontrolliertes Feedback | Nutzer*innen fordern die Kontrolle über KI-Interaktionen. | „Ich finde, sie sollte nur auf Anfrage erscheinen.“ (P, Z.147-148) „Mich stört bei vielen KIs in sozialen Medien [...], dass die die ganze Zeit angezeigt werden [...] aber genau wenn dann Misinformation im Post ist [...] dass man dann vielleicht eine kleine Anzeige hat, ‚Oh das könnte Misinformation sein‘ [...] und der dann zeigt, warum es Misinformationen sein könnte [...] und dass es strukturiert und übersichtlich ist und nicht zu überwältigend [...]" (P2, Z.116-124) |

Die Analyse der Interviews verdeutlicht, dass das Vertrauen in KI gestützte Systeme zur Erkennung von Desinformation maßgeblich von verschiedenen voneinander abhängenden Faktoren beeinflusst wird. Zu den zentralen Aspekten zählen dabei Transparenz, Neutralität, Nutzerkontrolle sowie eine klar geregelte Verantwortungsstruktur.

Im Hinblick auf die Transparenz erwarten Nutzer*innen, dass KI Systeme ihre Entscheidungen in nachvollziehbarer Weise erläutern können. Dies schließt insbesondere verständliche Begründungen, eine Offenlegung der Entscheidungsgrundlagen sowie transparente Quellenangaben mit ein. Eine solche technische Nachvollziehbarkeit ist essenziell, um Vertrauen in algorithmische Entscheidungen zu etablieren und die „Black-Box“-Wahrnehmung, die vielen KI Systemen anhaftet, zu vermeiden.

Darüber hinaus erweist sich die wahrgenommene Neutralität der Systeme als entscheidend. Nutzer*innen betonen, dass die KI nicht unter dem Einfluss privatwirtschaftlicher Interessen stehen darf. Eine unabhängige und möglichst objektive Entscheidungsfindung, die idealerweise unter staatlicher oder gemeinnütziger Aufsicht agiert, wird als Voraussetzung angesehen, um manipulative Verzerrungen zu verhindern und die ethische Integrität des Systems sicherzustellen.

Ein weiterer zentraler Aspekt betrifft die Nutzerkontrolle. Die Befragten bevorzugen Formen der Interaktion, bei denen sie selbst entscheiden können, inwieweit sie durch das System unterstützt oder gewarnt werden möchten. Statt bevormundender, automatisierter Eingriffe werden dezente, kontextabhängige Hinweise favorisiert, die den individuellen Bedarf berücksichtigen und die Autonomie der Nutzer*innen respektieren.

Hinsichtlich der Verantwortung wird deutlich, dass diese nicht ausschließlich bei den Entwickler* innen der Systeme oder den betreibenden Plattformen liegen kann. Vielmehr wird eine kollektive Verantwortung gefordert, die auch staatliche Institutionen sowie die Nutzer*innen selbst einschließt. Diese geteilte Verantwortung soll sicherstellen, dass sowohl technische als auch ethische Standards eingehalten und kontinuierlich weiterentwickelt werden.

Die Analyse zeigt somit, dass Vertrauen in KI gestützte Misinformationserkennung nur dann entstehen kann, wenn mehrere Bedingungen gleichzeitig erfüllt sind: eine transparente und nachvollziehbare Funktionsweise, institutionelle Unabhängigkeit, eine nutzerzentrierte Interaktionsgestaltung sowie ein klar definiertes, gemeinschaftlich getragenes Verantwortungsmodell. Erst das Zusammenspiel dieser Elemente schafft die Grundlage für eine akzeptierte und vertrauenswürdige Integration von KI in digitale Informationsumgebungen.


## Quantitative Methode
### Studiendesign und Ablauf
Zur Beantwortung der ersten Forschungsfrage(„Welche Faktoren beeinflussen das Vertrauen von Nutzerinnen in KI-gestützte Misinformation-Detection-Tools?“) wurde ein Mixed-Design-Methode gewählt [@fahrenberg2013].

Innerhalb der Teilnehmenden wird der Effekt von KI-Unterstützung untersucht, indem jede Person zunächst eine Baseline-Aufgabe ohne KI durchführt und anschließend dieselbe Aufgabe mit KI-Feedback erhält (within-subjects). Zwischen den Teilnehmenden wird der Typ der KI-Erklärung variiert: Die Hälfte erhält ein evaluatives System, das nur eine Einschätzung anzeigt, die andere Hälfte ein empfehlendes System, das zusätzlich eine Handlungsempfehlung gibt (between-subjects).

- Between-Subjects-Faktor: zufällige Zuweisung zu einer evaluativen versus empfehlenden KI-Variante.

- Within-Subjects-Faktor: jede Person klassifiziert zunächst zehn Posts ohne KI (Baseline) und anschließend zehn analoge Posts mit KI-Feedback.

Die Online-Erhebung wird vom 24. Mai bis 02. Juni 2025 aktiv und wird auf der institutsinternen Plattform  gehostet.  Der Umfragelink wurde einmalig im AStA-Forum sowie per E-Mail an persönliche Kontakte verbreitet (Convenience-Sampling). 

Zum Zeitpunkt der Analyse lagen unserer Projektgruppe 28 vollständige Datensätze aus der eigenen Rekrutierung vor. Da im Rahmen der Lehrveranstaltung SMNF mehrere Projektgruppen parallel denselben standardisierten Fragebogen einsetzen, werden die erhobenen Datensätze zu einem gemeinsamen Gesamtdatensatz zusammengeführt und gemeinsam ausgewertet.

Der chronologische Ablauf der quantitativen Studie (siehe dynamisches Diagramm unten) gliedert sich in mehrere aufeinanderfolgende Phasen:
```{mermaid}
---
config:
  theme: neo
  look: neo
---
flowchart TD
    n2["Start"] --> n6["Datenschutz & Einwilligung"]
    n6 -- Ja --> A["Pre-Test<br>Fragebogen"]
    n6 -- Nein --> n8["Ende"]
    A --> B["Baseline-Klassifizierung<br>ohne KI & Workload-Messung"]
    B --> C["Post-Baseline<br>Fragebogen"]
    C --> D{"Randomisierung<br>KI-Variante"}
    D -- evaluativ --> E["KI-gestützte Klassifikation<br>(10 Posts) & Workload-Messung"]
    D -- empfehlend --> E
    E --> F["Post-Test Fragebogen<br>(ATI, UEQ, HCT, DLR, SIPA)"]
    F --> G["Datensichtung<br>& Bereinigung"]
    G --> H["Statistische Analyse<br>(t-Test, Korrelation)"]
    H --> I["Interpretation<br>& Reporting"]

    classDef future fill:#FDEDEC,stroke:#CD6155
    classDef Aqua stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
    classDef Rose stroke:#FF5978, fill:#FFDFE5, color:#8E2236
    classDef past fill:#D6EAF8, stroke:#2E86C1
    classDef Sky stroke:#374D7C, fill:#E2EBFF, color:#374D7C
    classDef Ash stroke:#999999, fill:#EEEEEE, color:#000000
    classDef Peach stroke:#FBB35A, fill:#FFEFDB, color:#8F632D

    class n2 future,Aqua
    class n6 future,past
    class n8 Rose
    class A past,Sky
    class B,C,E,F,G past
    class H future,Sky,Ash,Peach
    class I future,Peach
```

Nach der Einholung der datenschutzrechtlichen Einwilligung und dem Ausfüllen eines Pre-Test-Fragebogens erfolgt zunächst die Baseline-Klassifizierung, bei der die Teilnehmenden zehn Social-Media-Beiträge ohne KI-Unterstützung bewerten und gleichzeitig eine Workload-Messung durchgeführt wird. Im Anschluss daran beantworten sie einen Post-Baseline-Fragebogen zur Erfassung erster Nutzereindrücke und kognitiver Belastung.

Daraufhin erfolgt die Randomisierung in zwei Between-Subjects-Bedingungen: Die Teilnehmenden werden zufällig einer entweder evaluativen oder empfehlenden KI-Variante zugewiesen. In der folgenden Phase erfolgt eine weitere Klassifikationsaufgabe von zehn Beiträgen – diesmal mit KI-Feedback – begleitet von einer erneuten Workload-Erhebung.

Zum Abschluss füllen die Teilnehmenden einen Post-Test-Fragebogen aus, welcher unter anderem die Skalen ATI, HCT, sowie weitere UX-relevante Maße umfasst. Abschließend erfolgt eine Datensichtung und -bereinigung, bevor die Ergebnisse mittels inferenzstatistischer Verfahren (z. B. t-Test, Korrelationsanalysen) ausgewertet und interpretiert werden.

### Stichprobe
Die Teilnehmenden für die quantitative Befragung wurden über einen Online-Rekrutierungskanal gewonnen, konkret über ein deutschsprachiges Studierendenforum der Universität zu Lübeck. Die dort veröffentlichte Einladung trug den Titel „Online-Studie: Erkennst du Desinformation, wenn du sie siehst? – 1 VP-Stunde MDI & Psychologie“.

Als formale Einschlusskriterien galten ein Mindestalter von 18 Jahren, ausreichende Deutschkenntnisse sowie die Teilnahme über ein Laptop, einen PC oder ein Tablet. Personen, die aktuell in der Lehrveranstaltung SMNF eingeschrieben waren, sowie jene, die über ein Smartphone teilnehmen wollten oder unvollständige Datensätze abgaben, wurden ausgeschlossen.

Obwohl die Häufigkeit der Social-Media-Nutzung nicht explizit als Teilnahmevoraussetzung kommuniziert wurde, wurde sie im Fragebogen erfasst (Antwortskala: nie, einmal im Monat oder weniger, mehrmals im Monat, mehrmals pro Woche, täglich).

Diese Variable wird im Rahmen der Datenanalyse als Kovariate berücksichtigt, da ein regelmäßiger Kontakt mit sozialen Medien vorausgesetzt wird, um die Einschätzung KI-gestützter Desinformationshinweise realitätsnah bewerten zu können. Zusätzlich wird eine Sensitivitätsanalyse durchgeführt, bei der nur Teilnehmende mit einer Nutzung von mindestens „mehrmals pro Woche“ berücksichtigt werden, um mögliche Verzerrungen durch geringe Exposition zu kontrollieren.

### Erhebungsinstrumente
Zur Erfassung relevanter psychologischer Konstrukte wurden validierte Skalen eingesetzt. Die Technikaffinität der Teilnehmenden wurde mittels der ATI-Skala (Affinity for Technology Interaction) gemessen (9 Items) [@franke-2018].
Die Skala wurde zur Dichotomisierung der Stichprobe anhand der Technikaffinität (Median-Split) herangezogen.

Als Zielvariable für das Vertrauen in KI-gestützte Klassifikation kam die Human-Computer-Trust-Skala (HCT) von @madsen2000 zum Einsatz (15 Items, bestehend aus drei Subskalen: Perceived Reliability, Technical Competence, Understandability). Die interne Konsistenz der Skala wurde überprüft (Cronbach’s α siehe Codebook).

Zusätzlich wurden weitere Instrumente zur Erfassung von Usability (System Usability Scale, SUS), User Experience (UEQ), subjektivem Workload (DLR-WAT) und Informationsverarbeitungsbewusstsein (SIPA) erhoben, die jedoch im Rahmen dieser Analyse nicht fokussiert ausgewertet werden.

### Datenanalyse
Zur Beantwortung der zentralen Forschungsfragen sind inferenzstatistische Analysen vorgesehen. 
Forschungsfrage 1 zielte darauf ab, Unterschiede im Vertrauen gegenüber KI-Systemen in Abhängigkeit von der Technikaffinität zu prüfen. Hierzu wurde ein zweiseitiger t-Test durchgeführt. Die unabhängige Variable (Technikaffinität) wurde dichotomisiert (≥ Median = hohe Technikaffinität), während als abhängige Variable der HCT-Gesamtscore (metrisch) verwendet wurde, welcher das allgemeine Vertrauen in das KI-System abbildet.

Forschungsfrage 2 untersuchte den Zusammenhang zwischen der subjektiv wahrgenommenen Arbeitsbelastung während der Informationsklassifikation (DLR_KI) und dem generellen Vertrauen in die vom KI-System getroffenen Klassifikationen (HCT-Gesamtscore). Da beide Variablen metrisch skaliert sind und kein normalverteiltes Antwortverhalten vorlag, wurde eine nichtparametrische Spearman-Rangkorrelation zur Analyse verwendet.

| Fragestellung 	| Test 	| UV / Gruppierung 	| AV 	|
|---	|---	|---	|---	|
| Unterschied im KI-Vertrauen je Technikaffinität 	| zweiseitiger t-Test (α = .05) 	| ATI dichotomisiert (≥ Median = hoch) 	| HCT-Gesamtscore 	|
| Zusammenhang zwischen der subjektiv wahrgenommenen Arbeitsbelastung während der Klassifikation und dem gesamten Vertrauen der Nutzer*innen in das KI-System 	| Spearman-Rangkorrelation 	| DLR_KI  (metrisch, 15 Items) 	| HCT-Gesamtscore (metrisch, 15 Items) 	|

Das Signifikanzniveau wird für alle inferenzstatistischen Tests auf α = .05 festgelegt. Zusätzlich sollen Effektstärken (Cohen’s d für den t-Test bzw. Pearson’s r für die Korrelationsanalyse) berechnet werden. Die statistischen Voraussetzungen der Verfahren (Normalverteilung, Varianzhomogenität) werden vor der Analyse überprüft [@shapiro-1965], [@levene1960].

### Deskriptive Ergebnisse
```{r child = "quantitative results.qmd"}
suppressPackageStartupMessages(library(kableExtra))
```

### Ethik
Die Teilnahme an der Studie erfolgte freiwillig und konnte jederzeit ohne Angabe von Gründen abgebrochen werden, ohne dass den Teilnehmenden daraus Nachteile entstanden. Alle erhobenen Daten wurden anonymisiert, verschlüsselt gespeichert und nach Projektabschluss vollständig gelöscht.

Zur Wahrung der Anonymität wurde für die Vergabe der Versuchspersonenstunden ein anonymisierter VP-Code verwendet. Dieser ermöglichte eine Zuordnung der Teilnahme zur Studienleistung ohne Rückschluss auf die erhobenen inhaltlichen Daten. Eine Verknüpfung zwischen personenbezogenen Informationen und den Antworten im Fragebogen war somit ausgeschlossen.


# Diskussion

# Code of Conduct

## Umgang mit Feedback, unterschiedlichen Perspektiven und Meinungsverschiedenheiten

In unserer Gruppe möchten wir demokratische Prinzipien, gegenseitigen Respekt und Offenheit wahren.\
Konstruktive Kritik wird als Chance zur Verbesserung gesehen und bei weiteren Gruppenentscheidungen berücksichtigt.\
Die Meinungen aller Gruppenmitglieder werden ernst genommen und gemeinsam diskutiert.\
Entscheidungen orientieren sich am Mehrheitsprinzip, wobei unterschiedliche Perspektiven ebenfalls Beachtung finden.

## Faire Aufteilung der Arbeitslast

Wir achten auf eine faire Aufgabenverteilung nach Fähigkeiten und Verfügbarkeit.\
Zuständigkeiten werden klar geregelt und regelmäßig überprüft.

## Verhalten in Bezug auf vereinbarte und verpflichtende Termine

Alle Teammitglieder verpflichten sich, zugesagte Aufgaben rechtzeitig zu erledigen.\
Verzögerungen oder Schwierigkeiten werden frühzeitig kommuniziert, um gemeinsam Lösungen zu finden.

## Einhaltung der wissenschaftlichen Integrität

Wir verpflichten uns zur Einhaltung der Grundsätze wissenschaftlichen Arbeitens, insbesondere zu Ehrlichkeit, Objektivität und Transparenz.\
Plagiate, Datenmanipulation sowie das Verschweigen von Interessenskonflikten sind ausgeschlossen.

## Verpflichtung zum Schutz von Daten und zur Wahrung ihrer Vertraulichkeit

Sensible Daten – insbesondere personenbezogene Informationen – werden vertraulich behandelt und nicht ohne Zustimmung weitergegeben.\
Die gesetzlichen Vorgaben zum Datenschutz werden beachtet.

## Nutzung und Kennzeichnung von AI Tools (z.B. ChatGPT)

Die Nutzung erfolgt ausschließlich zur sprachlichen Überarbeitung, für Inspirationen oder zur strukturellen Unterstützung.\
Die Nutzung solcher Hilfsmittel wird im Sinne wissenschaftlicher Transparenz kenntlich gemacht.

# Anhang 1 – Rekrutierungstext

**Online-Studie: Erkennst du Desinformation, wenn du sie siehst?**  
*1 VP-Stunde MDI & Psychologie*

Schon einmal über einen Post gestolpert und gedacht: „Stimmt das wirklich?“  
Wir möchten verstehen, wie Nutzerinnen und Nutzer solche Beiträge einschätzen – egal, ob Digital-Native oder gelegentlicher Scroller*in.

---

## Kurz & knapp

In einer ca. 60-minütigen Online-Aufgabe bewertest du ausgewählte, bewusst veränderte Social-Media-Beiträge. Du brauchst keine Vorkenntnisse – einfach deine Einschätzung zählt.

---

## Teilnahmevoraussetzungen

- **Mindestens 18 Jahre alt**
- **Du solltest Deutsch gut genug verstehen, um die Aufgaben bearbeiten zu können**
- **Laptop/PC oder Tablet (Smartphone leider nicht möglich)**
- **Ca. 60 Minuten Zeit**
- **Nicht aktuell in der Lehrveranstaltung SMNF an der Uni Lübeck eingeschrieben**

---

## Mitmachen

**Teilnahmelink:** [https://dsslab.hciuse.sh/study/pilot?groupId=gr-a4](https://dsslab.hciuse.sh/study/pilot?groupId=gr-a4)

---

## Vergütung

- **1 VP-Stunde für Studierende der Medieninformatik oder Psychologie an der Universität zu Lübeck**

Deine Angaben bleiben anonym und werden ausschließlich zu Forschungszwecken verwendet.

---

**Neugierig? Klick auf den Link und teste dein Urteilsvermögen!**
