---
title: "Wie entsteht Vertrauen in KI-gestützte Desinformationsfilter? Eine empirische Untersuchung"
author: 
  - Nazar Demianyk
  - Anastasiia Kopylova
  - Luka Kotyshchuk
  - Anna-Lena Petersen
  - Ian Stettinger
date: today
lang: de
format:
  html: default
  pdf:
    documentclass: article
    papersize: a4
    fontsize: 12pt
    number-sections: true
    fig-pos: 'H'
header-includes:
   - \usepackage{float}
execute:
  echo: false
  editor: visual
bibliography: references.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---

<!-- GitHub-Info -->

**GitHub Repository:** https://github.com/NazarD01/SMNF2025-Aufsatz-GruppeA4.git

```{r}
cat("Commit Hash:", system("git rev-parse HEAD", intern = TRUE))
```

# Einleitung
Die rasante Entwicklung künstlicher Intelligenz (KI) hat die digitale Informationslandschaft tiefgreifend verändert. Insbesondere KI-basierte Klassifikationssysteme zur Erkennung von Desinformation gewinnen zunehmend an Bedeutung, da sie potenziell dazu beitragen können, die Verbreitung falscher oder manipulativer Inhalte in sozialen Netzwerken einzudämmen. Der Einsatz solcher Technologien wirft jedoch grundlegende Fragen hinsichtlich ihrer Akzeptanz, ihrer ethischen Einbettung und vor allem ihres Vertrauenspotenzials auf. 

Vertrauen stellt eine zentrale Voraussetzung für die nachhaltige Integration digitaler Technologien in die alltägliche Mediennutzung dar. Ohne ausreichendes Vertrauen in KI-Systeme besteht die Gefahr, dass Nutzer*innen diese ignorieren, hinterfragen oder sogar ablehnen – ungeachtet ihrer technischen Leistungsfähigkeit. Gerade im Kontext von Desinformation, bei der kognitive Sicherheit und Glaubwürdigkeit eine Schlüsselrolle spielen, ist Vertrauen ein komplexes und vielschichtiges psychologisches Konstrukt. Es wird unter anderem beeinflusst durch wahrgenommene Transparenz, Nutzerkontrolle, institutionelle Unabhängigkeit und die technische Nachvollziehbarkeit der Entscheidungsprozesse. 

Die vorliegende Studie verfolgt das Ziel, relevante Einflussfaktoren auf das Vertrauen von Nutzer*innen in KI-gestützte Misinformation-Detection-Tools systematisch zu untersuchen. Dazu wurde ein Mixed-Methods-Design gewählt, welches qualitative und quantitative Daten trianguliert und sowohl individuelle Nutzereinschätzungen als auch experimentelle Ergebnisse berücksichtigt. Im Fokus stehen insbesondere psychologische Konstrukte wie Technikaffinität, subjektive Arbeitsbelastung und Human-Computer-Trust sowie nutzerseitige Bewertungen von Systeminteraktionen mit und ohne KI-Feedback. 

Ziel der Arbeit ist es, Erkenntnisse darüber zu gewinnen, unter welchen Bedingungen Vertrauen in KI-Systeme zur Desinformations-Erkennung entsteht und welche Gestaltungsprinzipien berücksichtigt werden müssen, um die Akzeptanz solcher Technologien zu fördern. Die Ergebnisse sollen als empirische Grundlage für die verantwortungsvolle Entwicklung und Anwendung KI-basierter Systeme im digitalen Informationsraum dienen und gleichzeitig einen Beitrag zur ethisch informierten Technikgestaltung leisten.

# Literaturübersicht

Im Rahmen der Literaturrecherche wurden drei wissenschaftliche Beiträge identifiziert, die wesentliche Anknüpfungspunkte zu den Forschungsfragen rund um Vertrauen in Künstliche Intelligenz (KI)-gestützte Desinformations-Erkennung bieten. Sie beleuchten unterschiedliche Perspektiven – von individuellen Wahrnehmungen bis hin zu gesellschaftlich-regulatorischen Rahmenbedingungen.

@hwang2017 zeigt, wie digitale Desinformation demokratische Prozesse untergräbt und wie technologische Fortschritte – insbesondere KI und Machine Learning – die Glaubwürdigkeit von Falschmeldungen erhöhen.

@iglesiaskeller2024 et al. warnen, dass staatliche Strategien nur dann Vertrauen schaffen, wenn KI-basierte Systeme transparent, grundrechtskonform und im Zusammenspiel mehrerer Akteure implementiert werden. Überzogene Sanktionen oder Intransparenz untergraben das Vertrauen.

@wang2023 belegt anhand einer groß angelegten US-Umfrage, dass individuelles Algorithmusvertrauen die Akzeptanz KI-gestützter Moderation maßgeblich prägt; liberal Eingestellte bewerten sie tendenziell positiver, technisch Versierte häufig kritischer.

Zusammenfassend verdeutlichen die drei Arbeiten, dass Vertrauen in KI-gestützte Desinformationsbekämpfung nicht nur von technischer Funktionalität, sondern auch von gesellschaftlichen, politischen und kommunikativen Rahmenbedingungen abhängt. Dieses Zusammenspiel ist entscheidend für die Weiterentwicklung und Akzeptanz von KI-Tools.

# Methode
## Qualitativer Forschungsansatz
Wir wählten einen qualitativen Ansatz, da unsere explorative Forschungs- frage ein vertieftes Verständnis bislang wenig untersuchter Nutzungsanfor- derungen verlangte. Neben technischen Dimensionen betrachteten wir soziale, ethische und Nutzer*innenbezogene Aspekte wie Vertrauen, Transparenz und Datenschutz – Faktoren, die stark kontextabhängig sind und sich am besten mit offenen Verfahren wie Interviews oder Fokusgruppen erfassen ließen. Diese Methoden ermöglichten es, auch unerwartete Perspektiven aufzudecken, die standardisierte Fragebögen nicht abbilden könnten.

### Stichprobe
Es wurden zwei Personen aus der Zielgruppe junger Erwachsener befragt, konkret handelte es sich um Studierende, die mittels eines Convenience Sampling ausgewählt wurden. Junge Erwachsene – insbesondere Studierende – wiesen eine regelmäßige Nutzung von Künstlicher Intelligenz (KI) auf und zeigten eine hohe Aktivität in sozialen Netzwerken. Dementsprechend waren sie häufig mit KI-generierten Inhalten konfrontiert, insbesondere im Kontext von Nachrichten. Aufgrund ihrer Altersstruktur sowie ihrer lebensweltlichen Nähe zum Thema wurde erwartet, dass sie reichhaltige und erfahrungsbasierte Einblicke in ihr Vertrauen gegenüber KI sowie ihren Bedarf an Unterstützung geben würden.


### Datenerhebung
Für die Erhebung entschieden wir uns für persönliche Face-to-Face-Interviews. Zu Beginn wurden die Interviewpersonen ausführlich über die Datenschutzbestimmungen sowie die Einwilligungserklärung informiert. Es wurde sichergestellt, dass sie dem Inhalt zustimmten.   Nach erfolgter Zustimmung begann das Interview. Die Fragen stammten überwiegend aus den bereitgestellten Materialien, wurden jedoch teils modifiziert oder präzisiert, um die Forschungsziele angemessen zu adressieren. Beide Interviews wurden mittels eines Diktiergeräts aufgezeichnet, anschließend mit dem KI-gestützten Dienst „Whisper“ transkribiert und die resultierenden Transkripte im Anschluss manuell überarbeitet.

### Datenanalyse
Im Rahmen der Auswertung wurde die thematische Analyse nach @braun_clarke gewählt. . Diese Methode umfasst sechs Phasen, welche schrittweise analysiert, beschrieben und zusammengefasst wurden. Die Codierung erfolgte zunächst unabhängig durch die einzelnen Teammitglieder. Anschließend wurden die individuellen Codes zusammengeführt und etwaige Abweichungen diskutiert, um eine konsistente Interpretation zu gewährleisten. Dieses Vorgehen mündete in der Identifikation von drei zentralen Themen, die im anschließenden Ergebnisabschnitt detailliert erläutert wurden.

### Ergebnisse
Wir führten zwei leitfadengestützte Interviews durch und analysierten diese auf Basis der Themenanalyse nach  @braun_clarke ausgewertet. Befragt wurden zwei Studierende aus der Zielgruppe junger Erwachsener, die aufgrund ihrer ausgeprägten digitalen Affinität und regelmäßigen Nutzung Künstlicher Intelligenz (KI) als besonders relevant für unsere Fragestellung gelten. KI begegnete ihnen in vielfältigen Kontexten – etwa in der akademischen Informationsrecherche oder bei alltäglichen Aufgaben – wodurch sie differenzierte Einschätzungen zur Funktionsweise, unterstützenden Rolle und zum Einfluss algorithmischer Systeme auf die Mediennutzung vornehmen konnten.
Insbesondere soziale Netzwerke, die von jungen Menschen intensiv genutzt werden, erwiesen sich als bedeutender Raum für KI-gestützte Informationsverbreitung, einschließlich potenziell irreführender Inhalte. Die hohe Technikaffinität der Studierenden machte sie sowohl anfällig für Desinformation als auch fähig zur kritischen Reflexion algorithmischer Logiken. Vor diesem Hintergrund hielten wir es für zentral, ihre Perspektiven zu berücksichtigen, da sie im Alltag verstärkt mit KI konfrontiert sind und individuelle Strategien im Umgang mit ihr entwickeln.

::: {#tbl-qualithemen .table tbl-cap="Themenübersicht qualitativer Kategorien mit Zitaten"}
| Thema | Definition | Zitate |
|---|---|---|
| Erklärbarkeit schafft Vertrauen | Transparente Offenlegung der KI-Entscheidungskriterien wird als essenziell eingestuft. | „Das System sollte logisch arbeiten und nachvollziehbar machen, weshalb es bestimmte Informationen auf eine bestimmte Weise eingeordnet hat.“ (P, Z.129-131) „Wenn ein Post Misinformation ist [...], es dann auch anzeigt, dass es Misinformation ist [...]." (P2, Z.72-75) |
| Neutralität und Unabhängigkeit | KI muss als unparteiisch wahrgenommen werden. | „Es darf keinerlei äußeren Einfluss haben.“ (P, Z.132) „Insbesondere große Konzerne sollten nicht zu viel Einfluss darauf haben.“ (P, Z.174-176) |
| Nutzerkontrolliertes Feedback | Nutzer*innen fordern die Kontrolle über KI-Interaktionen. | „Ich finde, sie sollte nur auf Anfrage erscheinen.“ (P, Z.147-148) „Mich stört bei vielen KIs in sozialen Medien [...], dass die die ganze Zeit angezeigt werden [...] aber genau wenn dann Misinformation im Post ist [...] dass man dann vielleicht eine kleine Anzeige hat, ‚Oh das könnte Misinformation sein‘ [...] und der dann zeigt, warum es Misinformationen sein könnte [...] und dass es strukturiert und übersichtlich ist und nicht zu überwältigend [...]" (P2, Z.116-124) |
:::


Die Analyse der durchgeführten Interviews verdeutlichte, dass das Vertrauen in KI-gestützte Systeme zur Erkennung von Desinformation maßgeblich durch verschiedene, miteinander verknüpfte Faktoren beeinflusst wurde. Zu den zentralen Aspekten zählten dabei Transparenz, Neutralität, Nutzerkontrolle sowie eine klar geregelte Verantwortungsstruktur.

Im Hinblick auf die Transparenz erwarteten die Nutzer*innen, dass KI-Systeme ihre Entscheidungen auf nachvollziehbare Weise erläutern konnten. Dies umfasste insbesondere verständliche Begründungen, die Offenlegung der Entscheidungsgrundlagen sowie transparente Quellenangaben. Diese technische Nachvollziehbarkeit erwies sich als essenziell, um Vertrauen in algorithmische Entscheidungen zu etablieren und die häufig als „Black Box“ wahrgenommene Intransparenz zu überwinden.

Darüber hinaus stellte sich die wahrgenommene Neutralität der Systeme als entscheidend heraus. Die Befragten betonten, dass die KI nicht dem Einfluss privatwirtschaftlicher Interessen unterliegen dürfe. Eine unabhängige und möglichst objektive Entscheidungsfindung, idealerweise unter staatlicher oder gemeinnütziger Aufsicht, wurde als notwendig angesehen, um manipulative Verzerrungen zu verhindern und die ethische Integrität der Systeme zu gewährleisten.

Auch die Nutzerkontrolle spielte eine zentrale Rolle. Die Befragten bevorzugten Interaktionsformen, die ihnen ermöglichten, selbst zu entscheiden, in welchem Umfang sie durch das System unterstützt oder gewarnt werden wollten. Statt bevormundender, automatisierter Eingriffe wurden dezente und kontextabhängige Hinweise präferiert, die individuellen Bedarfen Rechnung trugen und die Autonomie der Nutzer*innen respektierten.

Im Hinblick auf die Verantwortung zeigte sich, dass diese nicht allein bei den Entwickler\*innen oder Plattformbetreiber\*innen liegen konnte. Vielmehr wurde ein Modell geteilter Verantwortung gefordert, das staatliche Institutionen ebenso wie die Nutzer*innen selbst einschloss. Dieses Konzept zielte darauf ab, sowohl technische als auch ethische Standards zu sichern und kontinuierlich weiterzuentwickeln.

Insgesamt zeigte die Analyse, dass Vertrauen in KI-gestützte Systeme zur Desinformationsbekämpfung nur entstehen konnte, wenn mehrere Bedingungen gleichzeitig erfüllt waren: eine transparente und nachvollziehbare Funktionsweise, institutionelle Unabhängigkeit, nutzerzentrierte Interaktionsgestaltung und ein gemeinschaftlich getragenes Verantwortungsmodell. Erst das Zusammenspiel dieser Faktoren bildete die Grundlage für eine akzeptierte und vertrauenswürdige Integration von KI in digitale Informationsumgebungen.


## Quantitative Methode
### Studiendesign und Ablauf
Zur Beantwortung der ersten Forschungsfrage („Welche Faktoren beeinflussen das Vertrauen von Nutzer*innen in KI-gestützte Misinformation-Detection-Tools?“) wählten wir ein Mixed-Design-Methodendesign [@fahrenberg2013].

Innerhalb der Teilnehmendengruppe untersuchten wir den Effekt von KI-Unterstützung, indem jede Person zunächst eine Baseline-Aufgabe ohne KI absolvierte und anschließend dieselbe Aufgabe mit KI-Feedback bearbeitete (Within-Subjects-Design). Zwischen den Teilnehmenden variierten wir den Typ der KI-Erklärung: Eine Hälfte erhielt ein evaluatives System, das lediglich eine Einschätzung präsentierte, während der anderen Hälfte ein empfehlendes System zur Verfügung stand, dass zusätzlich eine Handlungsempfehlung beinhaltete (Between-Subjects-Design).

- Between-Subjects-Faktor: zufällige Zuweisung zu einer evaluativen versus empfehlenden KI-Variante.

- Within-Subjects-Faktor: jede Person klassifiziert zunächst zehn Posts ohne KI (Baseline) und anschließend zehn analoge Posts mit KI-Feedback.

Die Online-Erhebung war vom 24. Mai bis 02. Juni 2025 aktiv und wurde auf der institutsinternen Plattform gehostet. Die Teilnehmenden wurden mittels Convenience-Sampling rekrutiert: Die Umfrage wurde einmalig über das AStA-Forum sowie per E-Mail an persönliche Kontakte verbreitet.

Zum Zeitpunkt der Analyse lagen unserer Projektgruppe 28 vollständig ausgefüllte Datensätze aus der eigenen Rekrutierung vor. Da im Rahmen der Lehrveranstaltung SMNF mehrere Projektgruppen parallel denselben standardisierten Fragebogen eingesetzt hatten, wurden die erhobenen Datensätze zu einem gemeinsamen Gesamtdatensatz zusammengeführt und gemeinsam ausgewertet.

Der chronologische Ablauf der quantitativen Studie gliederte sich in mehrere aufeinanderfolgenden Phasen (siehe @fig-flussdiagramm).

![Ablauf der Studie im Flussdiagramm](123.png){#fig-flussdiagramm}

Nach der Einholung der datenschutzrechtlichen Einwilligung und dem Ausfüllen eines Pre-Test-Fragebogens erfolgte zunächst die Baseline-Klassifizierung. Dabei bewerteten die Teilnehmenden zehn Social-Media-Beiträge ohne KI-Unterstützung, wobei parallel eine Workload-Messung durchgeführt wurde. Im Anschluss beantworteten sie einen Post-Baseline-Fragebogen zur Erfassung erster Nutzereindrücke und kognitiver Belastung. Anschließend wurden die Teilnehmenden mittels Randomisierung einer von zwei Between-Subjects-Bedingungen zugewiesen: entweder einer evaluativen oder einer empfehlenden KI-Variante.

In der darauffolgenden Phase bearbeiteten die Teilnehmenden eine weitere Klassifikationsaufgabe mit zehn Social-Media-Beiträgen – diesmal unter Einbezug von KI-Feedback. Auch hier wurde erneut die individuelle Arbeitsbelastung erfasst.

Zum Abschluss füllten die Teilnehmenden einen Post-Test-Fragebogen aus, welcher unter anderem die Skalen ATI, HCT sowie weitere UX-relevante Maße beinhaltete. Abschließend erfolgte eine Datensichtung und -bereinigung. Die Auswertung und Interpretation der Daten wurde mithilfe inferenzstatistischer Verfahren durchgeführt, darunter t-Tests und Korrelationsanalysen.

### Stichprobe
Die Teilnehmenden für die quantitative Befragung wurden über einen Online-Rekrutierungskanal gewonnen, konkret über ein deutschsprachiges Studierendenforum der Universität zu Lübeck. Die dort veröffentlichte Einladung trug den Titel „Online-Studie: Erkennst du Desinformation, wenn du sie siehst? – 1 VP-Stunde MDI & Psychologie“.

Als formale Einschlusskriterien galten ein Mindestalter von 18 Jahren, ausreichende Deutschkenntnisse sowie die Teilnahme über ein Laptop, einen PC oder ein Tablet. Personen, die zum Zeitpunkt der Erhebung in der Lehrveranstaltung SMNF eingeschrieben waren, über ein Smartphone teilnehmen wollten oder unvollständige Datensätze einreichten, wurden ausgeschlossen.

Obwohl die Häufigkeit der Social-Media-Nutzung nicht explizit als Teilnahmevoraussetzung kommuniziert worden war, wurde sie im Fragebogen erhoben (Antwortskala: nie, einmal im Monat oder weniger, mehrmals im Monat, mehrmals pro Woche, täglich).

Diese Variable wurde im Rahmen der Datenanalyse als Kovariate berücksichtigt, da ein regelmäßiger Kontakt mit sozialen Medien als Voraussetzung galt, um die Einschätzung KI-gestützter Desinformationshinweise realitätsnah bewerten zu können. Zusätzlich wurde eine Sensitivitätsanalyse durchgeführt, bei der ausschließlich Teilnehmende mit einer Nutzung von mindestens „mehrmals pro Woche“ berücksichtigt wurden, um potenzielle Verzerrungen durch geringe Medienexposition zu kontrollieren.

### Erhebungsinstrumente
Zur Erfassung relevanter psychologischer Konstrukte setzten wir validierte Skalen ein. Die Technikaffinität der Teilnehmenden wurde mit der ATI-Skala (Affinity for Technology Interaction; 9 Items) nach [@franke-2018] gemessen.
Diese Skala diente zur Dichotomisierung der Stichprobe anhand eines Median-Splits.

Als Zielvariable für das Vertrauen in KI-gestützte Klassifikationsprozesse verwendeten wir die Human-Computer-Trust-Skala (HCT) nach @madsen2000, bestehend aus 15 Items, die drei Subskalen abbilden: Perceived Reliability, Technical Competence und Understandability. Die interne Konsistenz der Skala wurde überprüft (Cronbach’s $\alpha$ siehe Codebook).

Darüber hinaus erhoben wir weitere psychometrische Instrumente zur Erfassung von Usability (System Usability Scale, SUS), User Experience (UEQ), subjektiver Arbeitsbelastung (DLR-WAT) sowie Informationsverarbeitungsbewusstsein (SIPA). Diese wurden im Rahmen der vorliegenden Analyse jedoch nicht fokussiert ausgewertet.

### Datenanalyse
Zur Beantwortung der zentralen Forschungsfragen sind inferenzstatistische Analysen vorgesehen. 
Forschungsfrage 1 zielte darauf ab, Unterschiede im Vertrauen gegenüber KI-Systemen in Abhängigkeit von der Technikaffinität zu prüfen. Hierzu wurde ein zweiseitiger t-Test durchgeführt. Die unabhängige Variable (Technikaffinität) wurde dichotomisiert (≥ Median = hohe Technikaffinität), während als abhängige Variable der HCT-Gesamtscore (metrisch) verwendet wurde, welcher das allgemeine Vertrauen in das KI-System abbildet.

Forschungsfrage 2 untersuchte den Zusammenhang zwischen der subjektiv wahrgenommenen Arbeitsbelastung während der Informationsklassifikation (DLR_KI) und dem generellen Vertrauen in die vom KI-System getroffenen Klassifikationen (HCT-Gesamtscore). Da beide Variablen metrisch skaliert sind und kein normalverteiltes Antwortverhalten vorlag, wurde eine nichtparametrische Spearman-Rangkorrelation zur Analyse verwendet.

::: {#tbl-statfragen .table tbl-cap="Statistische Analysen je Fragestellung"}
| Fragestellung 	| Test 	| UV / Gruppierung 	| AV 	|
|---	|---	|---	|---	|
| Unterschied im KI-Vertrauen je Technikaffinität 	| zweiseitiger t-Test ($\alpha$ = .05) 	| ATI dichotomisiert (≥ Median = hoch) 	| HCT-Gesamtscore 	|
| Zusammenhang zwischen der subjektiv wahrgenommenen Arbeitsbelastung während der Klassifikation und dem gesamten Vertrauen der Nutzer*innen in das KI-System 	| Spearman-Rangkorrelation 	| DLR_KI  (metrisch, 15 Items) 	| HCT-Gesamtscore (metrisch, 15 Items) 	|
:::

Das Signifikanzniveau wird für alle inferenzstatistischen Tests auf $\alpha$ = .05 festgelegt. Zusätzlich sollen Effektstärken (Cohen’s d für den t-Test bzw. Spearman für die Korrelationsanalyse) berechnet werden. Die statistischen Voraussetzungen der Verfahren (Normalverteilung, Varianzhomogenität) werden vor der Analyse überprüft [@shapiro-1965; @levene1960].

### Datenanalyse Stichprobe
Durch eine Online-Umfrage wurde eine Stichprobe von Studierenden (*N* = 177) rekrutiert. Die Teilnehmenden wurden darüber informiert, dass die Umfrage den Anforderungen der EU-Datenschutz-Grundverordnung (DSGVO) entsprach und alle erhobenen Daten anonymisiert wurden. Zusätzlich wurde die Nutzungshäufigkeit sozialer Medien erhoben, um aktive Nutzerinnen und Nutzer zu identifizieren. Auf dieser Basis wurden vier Personen ausgeschlossen, da ihre Aktivität unterhalb des definierten Grenzwerts lag (Variable: *smfrequency* >= 3; Item: Nutzungshäufigkeit von Social Media [1 = nie, 5 = täglich]). Dies stellte sicher, dass die Stichprobe die Zielgruppe der regelmäßigen Social-Media-Nutzer\*innen korrekt abbildet (*M* = 4,9, *Mdn* = 5, *SD* = 0,4). Daher wurden insgesamt *N* = 172 in die weitere Analyse einbezogen. Das Durchschnittsalter der Stichprobe betrug *M* = 25,8 Jahre (*Mdn* = 22, *SD* = 10,4).

### Deskriptive Ergebnisse
Die deskriptiven Kennwerte der für die Analyse relevanten Variablen sind in Tabelle 3 dargestellt. Die Skalen weisen durchweg zufriedenstellende interne Konsistenzen auf (Cronbachs $\alpha \ge 0{,}80$). 

```{r}
#| child: "quantitative results.qmd"
```


### Inferenzstatistik 
Um mögliche gruppenspezifische Unterschiede im Vertrauen in KI-Tools zu analysieren, wurde ein zweiseitiger t-Test für unabhängige Stichproben durchgeführt. Die Technikaffinität (ATI) wurde anhand des Medians der Stichprobe in zwei Gruppen unterteilt: Teilnehmende mit einem ATI-Wert über dem Median wurden der Gruppe mit hoher Technikaffinität zugeordnet, während Personen mit einem ATI-Wert unterhalb des Medians in der Gruppe mit niedriger Technikaffinität erfasst wurden. Diese Gruppeneinteilung stellt sicher, dass die Analyse auf einer gleichmäßigen Verteilung der Technikaffinität basiert. 

```{r}
#| label: fig-dot
#| fig-cap: "Vertrauen (HCT) in Abhängigkeit von Technikaffinität"
#| fig-alt: "Dot-and-Whisker-Plot"
#| echo: false
ggplot(plot_t, aes(x = ATI_group, y = mean)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = .2) +
  labs(x = "Technikaffinität", y = "HCT-Gesamtscore") +
  theme_minimal()
```

Die Differenz der durchschnittlichen Vertrauenswerte in KI-Technologien (HCT) zwischen Personen mit hoher Technikaffinität (*M* = 5,0, *SD* = 0,0, n = 90) und niedriger Technikaffinität (*M* = 4,7, *SD* = 0,5, n = 82) war signifikant (t(170) = 5,42, p < .001, d = 0,87). Dies deutet darauf hin, dass Technikaffinität zwar einen Einfluss auf das Vertrauen in KI haben könnte, jedoch ist die praktische Bedeutung dieses Unterschieds fraglich. Die berechnete Effektstärke nach [@cohen-1988] beträgt d = 0,87, was auf einen großen Effekt hinweist. Allerdings zeigt die visuelle Analyse der Daten (@fig-dot), dass die Mittelwerte beider Gruppen nah beieinander liegen, was darauf hindeutet, dass der berechnete starke Effekt durch die geringe Streuung beeinflusst, wurde.  

In der Gruppe mit hoher Technikaffinität (n = 90) wurde eine Standardabweichung von 0 angegeben, was bedeutet, dass alle Teilnehmer exakt denselben Wert von 5,0 angegeben haben. Dies kann methodische Konsequenzen haben: 

Ein t-Test setzt voraus, dass beide Gruppen eine gewisse Streuung aufweisen. Eine SD von 0 bedeutet, dass keine Variation innerhalb der Gruppe vorhanden ist, was die Berechnung des Standardfehlers stark beeinflusst und die Aussagekraft des Tests einschränken kann. 

Wenn der wahre Effekt klein ist, könnte eine größere Stichprobe helfen, ihn klarer nachzuweisen. 

```{r}
#| label: fig-scat
#| fig-cap: "Zusammenhang Workload und Gesamtvertrauen"
#| fig-alt: "Scatter-Plot mit Regressionslinie"
#| fig-env: figure
#| fig-pos: H
#| echo: false
ggplot(data, aes(x = DLR_KI, y = HCT)) +
  geom_point(alpha = .6) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  labs(x = "DLR-Workload (KI)", y = "HCT-Gesamtscore") +
  theme_minimal()
```

Die grafische Darstellung (@fig-scat) zeigt die Zusammenhänge zwischen der subjektiven Arbeitsbelastung durch KI (DLR_KI) und dem generellen Vertrauen in die Klassifikationen des KI-Systems (HCT-Gesamtscore). Dabei sind die Werte beider Variablen als Einzelpunkte in einem Streudiagramm dargestellt. Die x-Achse repräsentiert die subjektiv wahrgenommene Arbeitsbelastung nach der Nutzung der KI, während die y-Achse das generelle Vertrauen in die Klassifikationen des Systems widerspiegelt. 

Zusätzlich zeigt die Abbildung eine Regressionslinie, die mögliche Zusammenhänge zwischen den beiden Variablen visualisiert. Die breite Streuung der Datenpunkte sowie der nahezu horizontale Verlauf dieser Linie deuten darauf hin, dass kein klarer Zusammenhang zwischen beiden Faktoren besteht. Das bedeutet, dass eine höhere mentale Belastung während der Informationsklassifikation nicht zwingend mit einem geringeren Vertrauen in das KI-System einhergeht. 

### Ethik
Die Teilnahme an der Studie erfolgte freiwillig und konnte jederzeit ohne Angabe von Gründen abgebrochen werden, ohne dass den Teilnehmenden daraus Nachteile entstanden. Alle erhobenen Daten wurden anonymisiert, verschlüsselt gespeichert und nach Projektabschluss vollständig gelöscht.

Zur Wahrung der Anonymität wurde für die Vergabe der Versuchspersonenstunden ein anonymisierter VP-Code verwendet. Dieser ermöglichte eine Zuordnung der Teilnahme zur Studienleistung ohne Rückschluss auf die erhobenen inhaltlichen Daten. Eine Verknüpfung zwischen personenbezogenen Informationen und den Antworten im Fragebogen war somit ausgeschlossen.


# Diskussion
Die Studie leistet einen empirischen Beitrag zur Beantwortung der zentralen Forschungsfrage, welche Faktoren das Vertrauen von Nutzer*innen in KI-gestützte Misinformation-Detection-Tools beeinflussen. Die Ergebnisse der qualitativen und quantitativen Analysen zeigen, dass Vertrauen nicht monokausal entsteht, sondern durch ein Zusammenspiel technischer, funktionaler und sozialer Faktoren geprägt wird. Insbesondere Merkmale wie Transparenz, Neutralität, Nutzerkontrolle und eine klar geregelte Verantwortungsstruktur erwiesen sich in den Interviews als entscheidend für die Vertrauensbildung. Auch die quantitative Befragung unterstreicht die Relevanz psychologischer Konstrukte wie Technikaffinität und subjektiver Arbeitsbelastung, wenngleich erwartete Zusammenhänge nicht durchweg empirisch belegt werden konnten.

Die Bedeutung dieser Befunde für die zentrale Fragestellung liegt in der Differenzierung des Vertrauensbegriffs und seiner operationalisierbaren Komponenten im Kontext KI-basierter Entscheidungsfindung. Indem sowohl individuelle Bewertungen als auch systemische Rahmenbedingungen einbezogen wurden, ermöglicht die Studie ein ganzheitlicheres Verständnis davon, unter welchen Bedingungen Nutzer*innen algorithmischen Systemen Vertrauen schenken. Damit werden nicht nur technische Aspekte adressiert, sondern auch soziale und ethische Anforderungen an die Systemgestaltung sichtbar gemacht.

Für die wissenschaftliche Diskussion ergeben sich daraus weitreichende Implikationen. Die Ergebnisse demonstrieren, dass Vertrauen als soziotechnisches Phänomen verstanden werden muss, das kontextabhängig variiert und durch wahrgenommene Kontrolle, Transparenz und institutionelle Einbettung beeinflusst wird. Dies unterstreicht die Notwendigkeit interdisziplinärer Evaluationsstandards, die über rein technische Performanzkriterien hinausgehen und ethische, nutzerzentrierte sowie soziale Faktoren systematisch integrieren. Dies stellt eine Herausforderung für die Forschung dar, da Vertrauen nicht eindimensional messbar ist, sondern durch multiple Indikatoren beschrieben werden sollte. Zukünftige Studien könnten hier ansetzen und differenziertere Messinstrumente entwickeln, die das Zusammenspiel technischer und sozialer Faktoren systematisch erfassen.

Auch für die Gesellschaft ergeben sich wichtige Implikationen. In einer digitalen Informationsumgebung, die zunehmend durch KI kuratiert wird, sind Nutzerinnen auf verlässliche, nachvollziehbare und ethisch fundierte Klassifikationssysteme angewiesen. Gerade im Kampf gegen Desinformation, die demokratische Diskurse unterminieren kann, spielen vertrauenswürdige technische Lösungen eine zentrale Rolle. Die Studie zeigt jedoch, dass bloße Effizienz nicht genügt - vielmehr müssen Systeme so gestaltet werden, dass Nutzer*innen ihre Funktionsweise verstehen und sich nicht entmündigt fühlen. Dies verlangt nach gestalterischer Sensibilität und partizipativer Technikentwicklung, bei der Bedürfnisse und Erwartungen der Zielgruppen ernst genommen werden.

## Methodenkritische Einordnung und zukünftige Forschungsansätze
Gleichzeitig weist die Studie drei zentrale Limitationen auf. Erstens besteht eine Stichprobenproblematik, da die qualitative Analyse auf nur zwei Interviews aus dem studentischen Milieu basiert, während die quantitative Erhebung (N=172) durch Convenience-Sampling (Studierendenforen/persönliche Netzwerke) eine überwiegend junge, digital affine Gruppe erfasste. Dies limitiert die Generalisierbarkeit der Ergebnisse. Zweitens gab es methodische Vereinfachungen, etwa die Verwendung eines Median-Splits für Gruppenvergleiche (t-Tests), die Varianz artifiziell reduziert. Zudem wurde die inferenzstatistische Analyse durch fehlende Streuung (SD=0) in einer Gruppe eingeschränkt. Drittens erfolgte eine statische Betrachtung, da die neuntägige Erhebungsphase Vertrauen lediglich als Momentaufnahme erfasste, obwohl es sich dynamisch durch wiederholte Interaktionen entwickelt.

Für zukünftige Forschung ergeben sich daraus drei Handlungsstränge. Erstens sollte eine Stichprobenvalidierung durch gezielte Rekrutierung diverser Alters- und Berufsgruppen via Online-Panels oder Community-Partizipation angestrebt werden. Zweitens sind Methodeninnovationen notwendig, insbesondere Längsschnittdesigns zur Erfassung dynamischer Vertrauensprozesse sowie die Vermeidung von Median-Splits zugunsten kontinuierlicher Analysemodelle. Drittens sollte die Systemgestaltung experimentell untersucht werden, beispielsweise durch erweiterte Erklärformate (visuell, personalisiert) unter realistischeren Nutzungsbedingungen.

## Fazit
Insgesamt liefert die Studie wertvolle Impulse für Forschung und Praxis zur vertrauensfördernden Gestaltung von KI-Systemen im Kontext der digitalen Desinformationsbekämpfung. Sie unterstreicht, dass Vertrauen weder vorausgesetzt noch automatisiert hergestellt werden kann, sondern durch sorgfältig konzipierte soziotechnische Arrangements aktiv gefördert werden muss. Die identifizierten Limitationen verdeutlichen zugleich den Bedarf an robusteren Designs, um das Verständnis von Vertrauensbildung in KI-Systemen weiter zu vertiefen.



# Code of Conduct

## Umgang mit Feedback, unterschiedlichen Perspektiven und Meinungsverschiedenheiten

In unserer Gruppe möchten wir demokratische Prinzipien, gegenseitigen Respekt und Offenheit wahren.\
Konstruktive Kritik wird als Chance zur Verbesserung gesehen und bei weiteren Gruppenentscheidungen berücksichtigt.\
Die Meinungen aller Gruppenmitglieder werden ernst genommen und gemeinsam diskutiert.\
Entscheidungen orientieren sich am Mehrheitsprinzip, wobei unterschiedliche Perspektiven ebenfalls Beachtung finden.

## Faire Aufteilung der Arbeitslast

Wir achten auf eine faire Aufgabenverteilung nach Fähigkeiten und Verfügbarkeit.\
Zuständigkeiten werden klar geregelt und regelmäßig überprüft.

## Verhalten in Bezug auf vereinbarte und verpflichtende Termine

Alle Teammitglieder verpflichten sich, zugesagte Aufgaben rechtzeitig zu erledigen.\
Verzögerungen oder Schwierigkeiten werden frühzeitig kommuniziert, um gemeinsam Lösungen zu finden.

## Einhaltung der wissenschaftlichen Integrität

Wir verpflichten uns zur Einhaltung der Grundsätze wissenschaftlichen Arbeitens, insbesondere zu Ehrlichkeit, Objektivität und Transparenz.\
Plagiate, Datenmanipulation sowie das Verschweigen von Interessenskonflikten sind ausgeschlossen.

## Verpflichtung zum Schutz von Daten und zur Wahrung ihrer Vertraulichkeit

Sensible Daten – insbesondere personenbezogene Informationen – werden vertraulich behandelt und nicht ohne Zustimmung weitergegeben.\
Die gesetzlichen Vorgaben zum Datenschutz werden beachtet.

## Nutzung und Kennzeichnung von AI Tools (z.B. ChatGPT)

Die Nutzung erfolgt ausschließlich zur sprachlichen Überarbeitung, für Inspirationen oder zur strukturellen Unterstützung.\
Die Nutzung solcher Hilfsmittel wird im Sinne wissenschaftlicher Transparenz kenntlich gemacht.

# Anhang 1 – Rekrutierungstext

**Online-Studie: Erkennst du Desinformation, wenn du sie siehst?**  
*1 VP-Stunde MDI & Psychologie*

Schon einmal über einen Post gestolpert und gedacht: „Stimmt das wirklich?“  
Wir möchten verstehen, wie Nutzerinnen und Nutzer solche Beiträge einschätzen – egal, ob Digital-Native oder gelegentlicher Scroller*in.

---

## Kurz & knapp

In einer ca. 60-minütigen Online-Aufgabe bewertest du ausgewählte, bewusst veränderte Social-Media-Beiträge. Du brauchst keine Vorkenntnisse – einfach deine Einschätzung zählt.

---

## Teilnahmevoraussetzungen

- **Mindestens 18 Jahre alt**
- **Du solltest Deutsch gut genug verstehen, um die Aufgaben bearbeiten zu können**
- **Laptop/PC oder Tablet (Smartphone leider nicht möglich)**
- **Ca. 60 Minuten Zeit**
- **Nicht aktuell in der Lehrveranstaltung SMNF an der Uni Lübeck eingeschrieben**

---

## Mitmachen

**Teilnahmelink:** [https://dsslab.hciuse.sh/study/pilot?groupId=gr-a4](https://dsslab.hciuse.sh/study/pilot?groupId=gr-a4)

---

## Vergütung

- **1 VP-Stunde für Studierende der Medieninformatik oder Psychologie an der Universität zu Lübeck**

Deine Angaben bleiben anonym und werden ausschließlich zu Forschungszwecken verwendet.

---

**Neugierig? Klick auf den Link und teste dein Urteilsvermögen!**

# Quellen